# Import Statements

import cv2
import keras
import numpy as np 
import pandas as pd 
import sklearn 
import tensorflow as tf
import pickle # Converts objects hierarchy to byte streams and vice versa
import warnings # To hide Python warnings to the user 
import math
import gdown # Imports large files from Google drive
import urllib.request # For accesing and importing URLs
from scipy.spatial import distance # Computing distances between arrays
from matplotlib import pyplot as plt 
import dlib
from sklearn.decomposition import PCA # Compress input using a scaling factor 
from sklearn.preprocessing import StandardScaler # Standardises by removing the mean and scaling
from sklearn.linear_model import LogisticRegression
from sklearn import tree 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import io 
import keras
from keras.models import Sequential
from keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D
from keras.wrappers.scikit_learn import KerasClassifier
import keras.optimizers as optimizers
from keras.applications.vgg16 import VGG16
from keras.models import Sequential
from keras.layers.core import Flatten, Dense, Dropout
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD
import sklearn.metrics as metrics
from tensorflow.keras.losses import categorical_crossentropy
from keras.callbacks import ModelCheckpoint 
import seaborn as sns 
dlibshape_url = 'https://drive.google.com/uc?id=17D3D89Gke6i5nKOvmsbPslrGg5rVgOwg'
dlibshape_path ='./shape_predictor_68_face_landmarks.dat'
gdown.download(dlibshape_url, dlibshape_path, True)
#Getting the Xpure loaded
pureX_url = 'https://drive.google.com/uc?id=1tc203qSeCwyayyFV7HHrNaMAdT__WoP5'
pureX_path = './pureX.npy'
gdown.download(pureX_url, pureX_path,True)
#Getting the Xdata loaded
dataX_url = 'https://drive.google.com/uc?id=1pQuzapUKRSXDNZqXuUUdJfO1L-AFuefY'
dataX_path = './dataX.npy'
gdown.download(dataX_url, dataX_path, True)
#Getting the Ydata loaded
dataY_url = 'https://drive.google.com/uc?id=1figaVbzcmrz8JumBs-1Ql20mbVIh_d0x'
dataY_path = './dataY.npy'
gdown.download(dataY_url, dataY_path, True)
dataset_url = 'https://drive.google.com/uc?id=1hNLedQCRl8uutOwtXWxBmiW3x7bckj4u'
dataset_path = './ferdata.csv'
gdown.download(dataset_url, dataset_path, True)

print ("Done")

# Helper Functions
frontalface_detector = dlib.get_frontal_face_detector()


def rect_to_bb(rect):
  x = rect.left()
  y = rect.top()
  w = rect.right() - x
  h = rect.bottom() - y 
  return (x,y,w,h)

def detect_face(image_url):
  try:
    url_response = urllib.request.urlopen(image_url)
    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)
    image = cv2.imdecode(img_array, -1)
  except Exception as e:
    return ("Sorry, image URL not found. Try again!")
  rects = frontalface_detector(image, 1)

  if len(rects) < 1:
    return ("No face detected")
  
  for (i, rect) in enumerate(rects):
    (x,y,w,h) = rect_to_bb(rect)
    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 225, 0), 2)

  plt.imshow(image, interpolation = 'nearest')
  plt.axis('off')
  plt.show()

print("Done")

def get_landmarks(image_url):
  
  try:
    url_response = urllib.request.urlopen(image_url)
    img_array = np.array(bytearray(url_response.read()), dtype=np.uint8)
    image = cv2.imdecode(img_array, -1)
    
  except Exception as e:
    print ("Please check the URL and try again!")
    return None,None
  
  faces = frontalface_detector(image, 1)
  if len(faces):
    landmarks = [(p.x, p.y) for p in landmark_predictor(image, faces[0]).parts()]
  else:
    return None,None
  
  return image,landmarks

def image_landmarks(image, face_landmarks):
  radius = -1
  circle_thickness = 5
  image_copy = image.copy()
  for (x, y) in face_landmarks:
     cv2.circle(image_copy, (x, y), circle_thickness, (0, 225, 0), 2)
  plt.imshow(image_copy, interpolation = "nearest")
  plt.axis("off")
  plt.show()

# Detecting Facial Landmarks

# https://i0.wp.com/bloggers.society19.com/wp-content/uploads/2015/11/Fresh-face-makeup.jpg?resize=564%2C688&ssl=1
# https://64.media.tumblr.com/6b8cec6b703684c8209c5f21e06d8221/tumblr_pocix7XvZx1s7h3f7o4_1280.jpg


# https://www.thefashionisto.com/wp-content/uploads/2013/07/J%C3%B8rn-476371.jpg
# http://scottstillman.com/blog/wp-content/uploads/2017/03/20170325_scan_Jill_8x10_e2.jpg
# https://www.clickinmoms.com/blog/wp-content/uploads/2014/10/black-and-white-portrait-of-man-with-his-eyes-closed-by-Brian-Powers.jpg
# https://i.pinimg.com/736x/a8/59/05/a85905aad4b379aafd63bbbd3144025d--freya-mavor-beautiful-people.jpg
# https://i.pinimg.com/236x/27/28/0e/27280ee28567c1e20c119f74981ee5c4--black-freckles-freckles-makeup.jpg
landmark_predictor = dlib.shape_predictor('./shape_predictor_68_face_landmarks.dat')
detect_face(input("Enter the URL of the image: "));

image, landmarks = get_landmarks(input("Enter the image's url: "))
if landmarks:
  image_landmarks(image, landmarks)
else:
  print("No landmarks detected")
print("Demonstrating machine's ability to detect specific feautures instead of all together: ")
print()
print("Detecting eyes: ")

eye_points = (36,47)
selected_landmarks = landmarks[eye_points[0]:eye_points[1]+1]
image_landmarks(image,selected_landmarks)
print("Detecting eyebrows: ")

eyebrow_points = (17,26)
selected_landmarks = landmarks[eyebrow_points[0]:eyebrow_points[1]+1]
image_landmarks(image,selected_landmarks)
print("Detecting nose: ")

nose_points = (27, 35)
selected_landmarks = landmarks[nose_points[0]:nose_points[1]+1]
image_landmarks(image,selected_landmarks)
print("Detecting mouth: ")

mouth_points = (48, 67)
selected_landmarks = landmarks[mouth_points[0]:mouth_points[1]+1]
image_landmarks(image,selected_landmarks)
print("Detecting jawline: ")

jawline_points = (0, 16)
selected_landmarks = landmarks[jawline_points[0]:jawline_points[1]+1]
image_landmarks(image,selected_landmarks)

# KNN MODEL

label_maps = {0:"ANGRY",1:"HAPPY",2:"SAD",3:"SURPRISE",4:"NEUTRAL"}
dataframe=pd.read_csv('./ferdata.csv')

preload = True 

if preload: 

  # load outputs saved in this folder after running preprocess_data() 
  dataX = np.load('./dataX.npy')
  dataY = np.load('./dataY.npy', allow_pickle=True)
  
else: 
  
  # this takes 15-20 minutes to run, but someone has already run it and saved the ouputs in this folder
  pureX, dataX, dataY = preprocess_data(df)
  
X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.1, random_state=42,stratify =dataY)
scaler = StandardScaler()
scaler.fit(X_train)
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)


pca = PCA(.95)
pca.fit(X_train)

X_train = pca.transform(X_train)
X_test= pca.transform(X_test)

knn = KNeighborsClassifier(n_neighbors=10)
print ("Training the knn model")
knn.fit(X_train, y_train)


print ("Predict for KNN Model")
y_predknn = knn.predict(X_test)
print ("Prediction Completed")
print ("Test Accuracy(KNN):",accuracy_score(y_test, y_predknn))

# LOGISTIC REGRESSION

trainer=LogisticRegression()
trainer.fit(X_train,y_train)
y_pred=trainer.predict(X_test)
accuracy=accuracy_score(y_test,y_pred)
print('Training for Logistic Regression model')
print('Prediction completed')
print('Test Accuracy(Logistic Regression):',accuracy)
print()

# DECISION TREE

clf=tree.DecisionTreeClassifier(random_state=0,max_depth=12,criterion='entropy')
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
accuracy=accuracy_score(y_test,y_pred)
print('Training Decision Tree model')
print('Prediction completed')
print('Test Accuracy(Decision Tree):',accuracy)
print()



